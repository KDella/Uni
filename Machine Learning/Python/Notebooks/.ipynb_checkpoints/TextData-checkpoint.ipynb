{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDjTGFQe8rLM"
   },
   "source": [
    "# Programming Exercise: Working with Text Data\n",
    "\n",
    "## Working with Text Data\n",
    "\n",
    "In this exercise, you will work with text data. Text data is usually represented as strings, made up of characters with variable length. This feature is clearly very different from the numeric features and we will need to process the data before we can apply our machine learning algorithms to it.\n",
    "\n",
    "## Applying Bag-of-Words to a Toy Dataset\n",
    "\n",
    "To construct a bag-of-words model based on the word counts in the respective documents, we can use the <samp>CountVectorizer</samp> class implemented in scikit-learn. As we will see in the following code section, the <samp>CountVectorizer</samp> class takes an array of text data, which can be documents or just sentences, and constructs the bag-of-words model for us: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E7QNM6c28rLO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9\n",
      "Vocabulary content:\n",
      " {'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "#Text data and Building Vocabulary\n",
    "import numpy as np\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining the weather is sweet and one and one is two'])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "count.fit(docs)\n",
    "print(\"Vocabulary size: {}\". format(len(count.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LoMmgHu8rLT"
   },
   "source": [
    "Fitting the <samp>CountVectorizer</samp> consists of the tokenization of the training data and building of the vocabulary, which we can access as the vocabulary\\_ attribute. In this case the vocabulary consists of 9 words.\n",
    "\n",
    "To create the bag-of-words representation for the training dataset, we call the <samp>transform</samp> method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GnYHqNR98rLU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words: <3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 17 stored elements in Compressed Sparse Row format>\n",
      "Dense representation of Bag of word:\n",
      " [[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#To create the bag-of-words representation\n",
    "bag = count.transform(docs)\n",
    "#Repr returns a string containing a printable representation of an object. \n",
    "print(\"Bag of words: {}\".format(repr(bag)))\n",
    "print(\"Dense representation of Bag of word:\\n {}\". format(bag.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JhxU9cg8rLY"
   },
   "source": [
    "The bag-of-words representation is stored in a sparse matrix that only stores the entries that are nonzero. A sparse matrix is used as most documents only contain a small subset of the words in the vocabulary. \n",
    "\n",
    "In the dense representation we have a bidimensional array of shape=*(number of sentences, number of words in vocabulary)*. In this case the element *(i, j)* of the array is the multiplicity of the word of the vocabulary with index *j* in the sentence *i*. For example, the first feature at index position 0 of the first raw resembles the count of the word \"and\", which only occurs in the last sentence, while the word \"is\" at the index position 1 (the 2nd feature in the vocabulary) occurs in all the three sentences, and in particular 3 times in the third.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foCJ8PBA8rLa"
   },
   "source": [
    "## Example Application: Sentiment Analysis of Movie Reviews\n",
    "\n",
    "In this exercise, we will use a dataset of movie reviews from the IMDb (Internet Movie Database). This dataset contains the text of the reviews, together with a label that indicates whether a review is \"positive\" or \"negative\".\n",
    "\n",
    "First, upload Train and Test data to server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oqmJfr6cx4p"
   },
   "source": [
    "Next, import data using `pandas`, a Python Data Analysis Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-NxqWzf8rLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Elements of train set:\n",
      "                                               review  sentiment\n",
      "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
      "1  OK... so... I really like Kris Kristofferson a...          0\n",
      "2  ***SPOILER*** Do not read this, if you think a...          0\n",
      "First Elements of test set:\n",
      "                                               review  sentiment\n",
      "0  I have seen several comments here about Brando...          1\n",
      "1  I liked this film very much. The story jumps b...          1\n",
      "2  There's a part of me that would like to give t...          0\n"
     ]
    }
   ],
   "source": [
    "#Movie reviews Dataset\n",
    "import pandas as pd\n",
    "train = pd.read_csv('http://ailab.uniud.it/wp-content/uploads/2019/05/Train_Movie_Data.csv')\n",
    "print(\"First Elements of train set:\\n {}\".format(train.head(3)))\n",
    "text_train = train['review'].values\n",
    "y_train = train['sentiment'].values\n",
    "test = pd.read_csv('http://ailab.uniud.it/wp-content/uploads/2019/05/Test_Movie_Data.csv')\n",
    "print(\"First Elements of test set:\\n {}\".format(test.head(3)))\n",
    "text_test = test['review'].values\n",
    "y_test = test['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wz-Bl8XE8rLe"
   },
   "source": [
    "Now, you will build the vocabulary and the bag-of-word representation of the <samp>text\\_train</samp>.\n",
    "\n",
    "#### Implementation Notes:\n",
    "<ul>\n",
    "    <li> Use the Class <samp>CountVectorizer</samp></li>\n",
    "    <li> Build the vocabulary using the training set </li>\n",
    "    <li> Compute the bag-of-words representation of <samp>text_train</samp> into <samp>X_train</samp> </li> \n",
    "    <li> Print the shape of <samp>X_train</samp> using: <samp>print(\"X_train:\\n{}\".format(repr(X_train)))</samp> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIXLJuCO8rLf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<24998x76850 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3408338 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "#Building the vocabulary and the bag of words\n",
    "# HERE YOUR CODE\n",
    "count = CountVectorizer()\n",
    "count.fit(text_train)\n",
    "X_train = count.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9nNK0xt8rLi"
   },
   "source": [
    "The shape of <samp>X_train</samp> is $24998\\times76850$, indicating that the vocabulary contains 76,850 entries.\n",
    "\n",
    "Let's look at the vocabulary in a bit more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "937BC_iJ8rLi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 76850\n",
      "First 20 features:\n",
      "['00', '000', '0000000000001', '00000001', '00015', '001', '007', '0079', '007s', '0080', '0083', '009', '0093638', '00am', '00o', '00pm', '00s', '01', '0148', '02']\n",
      "Features 20010 to 20030:\n",
      "['dozor', 'doña', 'doğan', 'dp', 'dpm', 'dpp', 'dq', 'dr', 'draaaaaaaawl', 'draaaaaags', 'drab', 'drablow', 'drably', 'drabness', 'drac', 'dracht', 'dracula', 'draculas', 'draft', 'drafted', 'draftee', 'draftees', 'drafthouse', 'drafting', 'drafts', 'drag', 'dragan', 'dragged', 'dragging', 'draggy']\n",
      "Every 2000th feature:\n",
      "['00', 'affiliation', 'approxiately', 'barbara', 'blobs', 'buoyancy', 'charitable', 'commentors', 'crippling', 'demotic', 'dolous', 'elysee', 'eyelid', 'follows', 'ghettos', 'gwyenths', 'hogue', 'indefinitely', 'jessie', 'kramp', 'liz', 'marketability', 'ministrations', 'nakedly', 'offsetting', 'pater', 'poiter', 'punishes', 'reimbursed', 'rosyton', 'seamen', 'single', 'spleen', 'sumida', 'testicle', 'triggered', 'unromantic', 'wanderlust', 'yaaa']\n"
     ]
    }
   ],
   "source": [
    "#Let's look at the vocabulary in a bit more detail.\n",
    "feature_names = count.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20400:20430]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1iK3ixau8rLl"
   },
   "source": [
    "As you can see, possibly a bit surprisingly, the first 20 entries in the vocabulary are all numbers. All these numbers appear somewhere in the reviews, and are therefore extracted as words. \n",
    "\n",
    "Once we have our feature, let's obtain a qualitative measure of performance by actually building a classifier. We have the training labels stored in <samp>y_train</samp> and the bag-of-words representation of the training data in <samp>X_train</samp>, so we can train a classifier on this data. For high-dimensional, sparse data like this, linear models like <samp>LogisticRegression</samp> could work best. \n",
    "\n",
    "Let's start by evaluating <samp>LogisticRegression</samp>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phUV5LBC8rLm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.00\n",
      "Test score: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "# HERE YOUR CODE \n",
    "#Use fit(X,y) fuction of LogisticRegression()to train your model, \n",
    "#where X is the array of your training data and y is the label \n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Compute the Bag of word representation of the testing set\n",
    "# HERE YOUR CODE\n",
    "X_test = count.transform(text_test)\n",
    "\n",
    "# HERE YOUR CODE\n",
    "#Use score(X,y) function of LogisticRegression() to compute \n",
    "#the performance on both training and testing set \n",
    "print(\"Train score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-dHL4sK8rLo"
   },
   "source": [
    "Based on these results we can see our model overfit the data. The LogisticRegression has a regularization parameter, C, which can tune, via a grid search strategy, to reduce the overfitting effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tk3GMm6P8rLq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n",
      "Test score: 0.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None, param_grid={'C': [0.1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "#For computation issue in this laboratory we test just one case  \n",
    "param_grid = {'C': [0.1]}\n",
    "\n",
    "def grid_search(X_train,y_train,X_test,y_test,param_grid):\n",
    "    grid = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "    print(\"Best parameters: \", grid.best_params_)\n",
    "    # Test on the Testing set\n",
    "    print(\"Test score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "    return grid\n",
    "\n",
    "\n",
    "grid_search(X_train, y_train, X_test, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elS0znzO8rLu"
   },
   "source": [
    "We obtain an accuracy of 89%, which indicates reasonable performance for a balanced binary classification task. Note the accuracy on the test set is the same of the previous test, but now the model does not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_AlJMS_Y8rLv"
   },
   "source": [
    "### Word with Multiple Appearances\n",
    "\n",
    "To clean the vocabulary from no-meaningful \"words\" we can use a simple mechanism that works quite well in practice: only use tokens that appear only at least two documents (or at least five documents, and so on). A token that appears only in a single document is unlikely to appear in the test set and is therefore not helpful. We can set the minimum number of documents a token needs to appear in with the <samp>min\\_df</samp> parameter (see below).\n",
    "\n",
    "By requiring at least five appearances of each token, we can bring down the number of feature to 27,039 - only about a third of the original features. There are clearly many fewer numbers, and some of the more obscure words seem to have vanished. \n",
    "\n",
    "The validation accuracy is unchanged from before. We did not improve our model, but having fewer features to deal with speeds up processing and throwing away useless features might make the model more interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27_geQhY8rLv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <24998x76850 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3408338 stored elements in Compressed Sparse Row format>\n",
      "Number of features: 27039\n",
      "First 50 features:\n",
      "['00', '000', '007', '01', '02', '03', '05', '06', '07', '08', '09', '10', '100', '1000', '100s', '100th', '101', '102', '103', '104', '105', '107', '108', '109', '10s', '10th', '11', '110', '111', '115', '116', '117', '11th', '12', '120', '1200', '123', '12th', '13', '130', '13th', '14', '140', '14th', '15', '150', '15th', '16', '160', '16mm']\n"
     ]
    }
   ],
   "source": [
    "#1) Set minimum number of documents a token needs to appear\n",
    "#Building the vocabulary and the bag of words\n",
    "# HERE YOUR CODE\n",
    "count = CountVectorizer(min_df=5)\n",
    "count.fit(text_train)\n",
    "X_Train = count.transform(text_train)\n",
    "\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))\n",
    "feature_names = count.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "\n",
    "X_test = count.transform(text_test)\n",
    "grid_search(X_train,y_train, X_test, y_test,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_r0wdml8rLx"
   },
   "source": [
    "### Advanced Tokenization\n",
    "\n",
    "The <samp>CountVectorize</samp> is relatively simple, but it could be improved using external methods.\n",
    "One particular step that is often improved in more sophisticated text-processing applications is the first step in the bag-of-words model: tokenization. This step defines what constitutes a word for the purpose of feature extraction. \n",
    "We saw earlier that the vocabulary often contains singular and plural version of some words: \"drawback\" and \"drawbacks\" or \"dracula\" and \"draculas\". For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only increase overfitting, and not allow the model to fully exploit the training data. \n",
    "\n",
    "This problem can be overcome by representing each word using its <samp>word stem</samp>, which involves identifying all the words that have the same word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as <samp>stemming</samp>. If instead a dictionary of known word is used, and the role of the word in\n",
    "the sentence is taken into account, the process is referred to as <samp>lemmatization</samp> and the standardized form of the word is referred to as the <samp>lemma</samp>.\n",
    "However, <samp>lemmatization</samp> is computationally more difficult and expensive compared to <samp>stemming</samp> and it could have little impact on the performance. \n",
    "The Natural Language Toolkit for Python (NLTK, http://www.nltk.org) implements the Snowball stemming algorithm, which we will use in the following code section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SmB1mCiytOZ"
   },
   "outputs": [],
   "source": [
    "# Here the Advanced Italian Tokenization  \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"italian\")\n",
    "def tokenizer_snowballStemmer(text):\n",
    "    return [stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_snowballStemmer(\"È possibile attivare la consultazione e la ricerca nei testi anche basate sulla lemmatizzazione\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8PCyWgwy6nL"
   },
   "outputs": [],
   "source": [
    "# Here the Advanced English Tokenization\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenizer_snowballStemmer(text):\n",
    "    return [stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_snowballStemmer(\"runners like running and thus they run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OdAErKoOb3Fr"
   },
   "source": [
    "Using the Snowball stemmer from the nltk package, we can classify the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHD_EswUb345"
   },
   "outputs": [],
   "source": [
    "#Classification with Tokenizer NLTK\n",
    "nltk_count = CountVectorizer(tokenizer=tokenizer_snowballStemmer, min_df=5).fit(text_train)\n",
    "X_train_nltk = nltk_count.transform(text_train)\n",
    "print(\"X_train_nltk: {}\".format(X_train_nltk.shape))\n",
    "\n",
    "X_test_nltk = nltk_count.transform(text_test)\n",
    "grid_search(X_train_nltk,y_train,X_test_nltk, y_test,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Quo6y8yqaRYs"
   },
   "source": [
    "### Rescaling the Data with tf-idf\n",
    "\n",
    "One of the most common approach to represent text is using <i>term frequency-inverse document frequency</i> (tf-idf) method. The intuition of this method is to give high weight to any term that appears often in a particular document, but not in many documents in the corpus. If a word appears often in a particular document, but not in very many documents, it is likely to be very descriptive of the content of that document. <i>scikit-learn</i> implements the tf-idf method in a class: <samp>TfidfVectorizer</samp>, which takes in the text data and does both the bag-of-words feature extraction and the tf-idf transformation. There are several variants of the tf-idf rescaling schema (see wikipedia). The tf-idf score for word $w$ in document $d$ as implemented in <samp>TfidfVectorizer</samp> class is given by: \n",
    "\\begin{equation}\n",
    " tfidf(w,d) = tf * \\left(\\ln\\left( \\frac{N+1}{N_w+1}\\right)+1\\right)\n",
    "\\end{equation}\n",
    "where $N$ is the number of documents in the training set, $N_w$ is the number of documents in the training set that the word $w$ appears, and $tf$ (the term frequency) is the number of times that the word $w$ appears in the query document $d$ (the document you want to transform or encode). The class also applies L2 normalization after computing the tf-idf representation; in other words, it rescales the representation of each document to have Euclidean length (this simply means each row is divided by its sum of squared entries). Rescaling in this way means that the length of a document (the number of words) does not change the vectorized representation. Test it completing the function <samp>tf_id_example</samp> using the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDI7KuEoaRYz"
   },
   "outputs": [],
   "source": [
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining the weather is sweet and one and one is two'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count = TfidfVectorizer()\n",
    "count.fit(docs)\n",
    "print(\"Vocabulary size: {}\". format(len(count.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count.vocabulary_))\n",
    "\n",
    "#To create the bag-of-words representation\n",
    "bag = count.transform(docs)\n",
    "print(\"Bag of words: {}\".format(repr(bag)))\n",
    "print(\"Dense representation of Bag of word:\\n {}\". format(bag.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uqV1foaaRY8"
   },
   "source": [
    "Now, you will adapt this code for the movie reviews dataset. \n",
    "Keep in mind that the tf-idf scaling is meant to find words that distinguish documents, but it is a purely unsupervised technique. So, \"important\" here does not necessarily relate to the \"positive review\" and \"negative review\" label we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkMjG4qJaRY9"
   },
   "outputs": [],
   "source": [
    "countTFIDF = TfidfVectorizer(min_df=5,tokenizer=tokenizer_snowballStemmer).fit(text_train)\n",
    "#HERE YOUR CODE\n",
    "#Building TF-IDF bag of word represnetation\n",
    "#XTFIDF_train = countTFIDF.XXX\n",
    "#XTFIDF_test = countTFIDF.XXX\n",
    "print(\"X_train:\\n{}\".format(repr(XTFIDF_train)))\n",
    "\n",
    "grid=grid_search(XTFIDF_train, y_train, XTFIDF_test, y_test,param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiIoF4gzaRZA"
   },
   "source": [
    "### Investigating Model Coefficients\n",
    "\n",
    "Finally, let's look in a bit more detail into what our logistic regression model actually learned from the data. Because there are so many features we clearly cannot look at all of the coefficients at the same time. \n",
    "However, we can look at the largest coefficients, and see which words these correspond to. \n",
    "\n",
    "The following bar char show the largest and smallest coefficients of the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uYfUZ_22aRZB"
   },
   "outputs": [],
   "source": [
    "# Show coefficients\n",
    "################################################################################\n",
    "from matplotlib.colors import ListedColormap, colorConverter, LinearSegmentedColormap\n",
    "cm = ListedColormap(['#0000aa', '#ff2020'])\n",
    "\n",
    "\n",
    "def visualize_coefficients(coefficients, feature_names, n_top_features=25):\n",
    "    \"\"\"Visualize coefficients of a linear model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    coefficients : nd-array, shape (n_features,)\n",
    "        Model coefficients.\n",
    "    feature_names : list or nd-array of strings, shape (n_features,)\n",
    "        Feature names for labeling the coefficients.\n",
    "    n_top_features : int, default=25\n",
    "        How many features to show. The function will show the largest (most\n",
    "        positive) and smallest (most negative)  n_top_features coefficients,\n",
    "        for a total of 2 * n_top_features coefficients.\n",
    "    \"\"\"\n",
    "    coefficients = coefficients.squeeze()\n",
    "    if coefficients.ndim > 1:\n",
    "        # this is not a row or column vector\n",
    "        raise ValueError(\"coeffients must be 1d array or column vector, got\"\n",
    "                         \" shape {}\".format(coefficients.shape))\n",
    "    coefficients = coefficients.ravel()\n",
    "\n",
    "    if len(coefficients) != len(feature_names):\n",
    "        raise ValueError(\"Number of coefficients {} doesn't match number of\"\n",
    "                         \"feature names {}.\".format(len(coefficients),\n",
    "                                                    len(feature_names)))\n",
    "    # get coefficients with large absolute values\n",
    "    coef = coefficients.ravel()\n",
    "    positive_coefficients = np.argsort(coef)[-n_top_features:]\n",
    "    negative_coefficients = np.argsort(coef)[:n_top_features]\n",
    "    interesting_coefficients = np.hstack([negative_coefficients,\n",
    "                                          positive_coefficients])\n",
    "    # plot them\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = [cm(1) if c < 0 else cm(0)\n",
    "              for c in coef[interesting_coefficients]]\n",
    "    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients],\n",
    "            color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * n_top_features),\n",
    "               feature_names[interesting_coefficients], rotation=60,\n",
    "               ha=\"right\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "feature_names = np.array(countTFIDF.get_feature_names())\n",
    "sorted_by_idf = np.argsort(countTFIDF.idf_)\n",
    "print(\"Features wtih lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:100]]))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "visualize_coefficients(\n",
    "        grid.best_estimator_.coef_,\n",
    "        feature_names, n_top_features=40)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUKPIU0gaRZC"
   },
   "source": [
    "The negative coefficients on the left belong to words that according to the model are indicative of negative reviews, while the positive coefficients on the right belong to words that according to the model indicate positive reviews. Most of the terms are quite intuitive, like \"worst\", \"bad\" indicating bad movie reviews, while \"great\", \"enjoy\" indicate positive movies reviews. \n",
    "The <i>mglearn</i> is a library for plotting data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Working with Text - AERO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
